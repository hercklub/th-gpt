# PokerGPT Training with LLaMA 3.1 8B

This repository provides a complete training pipeline for PokerGPT using **LLaMA 3.1 8B** with DeepSpeed optimization. It implements a three-step RLHF training process optimized for RunPod.io deployment.

## ğŸš€ Quick Start

**New to the project? Start here:**

1. **[QUICK_START.md](QUICK_START.md)** - Complete step-by-step training guide
2. **[TRAINING_CHEATSHEET.md](TRAINING_CHEATSHEET.md)** - Quick command reference

### One-Command Setup

```bash
# On RunPod or any CUDA-enabled machine
bash setup_runpod.sh
```

### One-Command Training

```bash
# Train all 3 steps
bash run_training_pipeline.sh
```

## ğŸ“‹ What's New - LLaMA 3.1 8B Update

This repository is based on DeepSpeed-Chat and has been updated to support:

- âœ… **LLaMA 3.1 8B** (upgraded from OPT-1.3B)
- âœ… **RunPod.io** deployment ready
- âœ… **LoRA training** for memory efficiency
- âœ… **Automated setup** scripts
- âœ… **Interactive evaluation** tools
- âœ… **Comprehensive documentation**

### Training Pipeline

The training consists of 3 steps:

1. **Step 1: Supervised Fine-tuning (SFT)**
   - Train LLaMA 3.1 8B on poker gameplay data
   - Supports LoRA for efficient training

2. **Step 2: Reward Model Fine-tuning**
   - Train a reward model to evaluate poker decisions
   - Uses paired good/bad response comparisons

3. **Step 3: RLHF (PPO) Fine-tuning**
   - Use Proximal Policy Optimization
   - Align model with optimal poker strategy

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ step1_supervised_finetuning/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€â”€ training_scripts/single_node/
â”‚   â”‚       â”œâ”€â”€ run_llama_8b.sh           # Full fine-tuning
â”‚   â”‚       â””â”€â”€ run_llama_8b_lora.sh      # LoRA training (recommended)
â”‚   â”œâ”€â”€ step2_reward_model_finetuning/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€â”€ training_scripts/single_node/
â”‚   â”‚       â””â”€â”€ run_llama_8b.sh
â”‚   â”œâ”€â”€ step3_rlhf_finetuning/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€â”€ training_scripts/single_node/
â”‚   â”‚       â””â”€â”€ run_llama_8b.sh
â”‚   â””â”€â”€ utils/                             # Shared utilities
â”‚       â”œâ”€â”€ model/
â”‚       â”œâ”€â”€ data/
â”‚       â””â”€â”€ utils.py                       # Updated tokenizer handling
â”œâ”€â”€ local_data/                            # Data processing scripts
â”‚   â”œâ”€â”€ unzip_script.py
â”‚   â”œâ”€â”€ order_change.py
â”‚   â”œâ”€â”€ good_players.py
â”‚   â””â”€â”€ prompt_engineering3.py
â”œâ”€â”€ setup_runpod.sh                        # Automated environment setup
â”œâ”€â”€ run_training_pipeline.sh               # Run all 3 steps
â”œâ”€â”€ evaluate_model.py                      # Model evaluation tool
â”œâ”€â”€ requirements.txt                       # Python dependencies
â”œâ”€â”€ QUICK_START.md                         # Detailed training guide
â””â”€â”€ TRAINING_CHEATSHEET.md                # Quick command reference
```

## ğŸ”§ Requirements

- **GPU**: Minimum 24GB VRAM (40GB+ recommended)
- **RAM**: 32GB+ system RAM
- **Storage**: 200GB+ free space
- **Python**: 3.8+
- **CUDA**: 11.8+

See [QUICK_START.md](QUICK_START.md#resource-requirements) for detailed requirements.

## ğŸ“Š Training Overview

| Step | Model | Duration (A40) | GPU Memory |
|------|-------|----------------|------------|
| Step 1 (SFT LoRA) | LLaMA 3.1 8B | 4-8 hours | ~30GB |
| Step 2 (Reward) | LLaMA 3.1 8B | 2-4 hours | ~35GB |
| Step 3 (RLHF) | Both models | 6-12 hours | ~40GB |

## ğŸ¯ Key Features

- **Memory Efficient**: LoRA training reduces memory requirements by 50%
- **DeepSpeed ZeRO**: Distributed training optimization
- **Gradient Checkpointing**: Enabled by default for memory savings
- **Mixed Precision**: FP16 training for faster convergence
- **Flexible Configuration**: Easy-to-modify training scripts
- **Comprehensive Logging**: Track training progress in real-time

## ğŸ“– Documentation

- **[QUICK_START.md](QUICK_START.md)** - Complete training guide with:
  - RunPod setup instructions
  - Step-by-step training walkthrough
  - Configuration options
  - Troubleshooting guide
  - Resource requirements

- **[TRAINING_CHEATSHEET.md](TRAINING_CHEATSHEET.md)** - Quick reference for:
  - Common commands
  - Key parameters
  - File locations
  - Quick fixes

## ğŸ” Model Evaluation

After training, evaluate your model:

```bash
# Interactive evaluation
python evaluate_model.py \
    --model_path training/step3_rlhf_finetuning/output_llama_8b_rlhf \
    --mode interactive

# Batch evaluation
python evaluate_model.py \
    --model_path training/step3_rlhf_finetuning/output_llama_8b_rlhf \
    --mode batch \
    --test_file test_prompts.txt
```

## ğŸ² Data Processing

Original data processing pipeline (in `local_data/`):

1. **Unzip datasets**: `python unzip_script.py`
2. **Filter showdown games**: `python order_change.py`
3. **Filter good players**: `python good_players.py`
4. **Prompt engineering**: `python prompt_engineering3.py`

Final prompt files start with `prompt_*`.

## ğŸ’° Cost Estimates (RunPod Spot)

- **Full training pipeline**: $7-15 on A40 (48GB)
- **Single step testing**: $2-5
- **Total storage**: ~70GB (200GB recommended)

See [QUICK_START.md](QUICK_START.md#resource-requirements) for detailed pricing.

## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“„ License

Based on DeepSpeed-Chat. See original repository for license details.

## ğŸ™ Acknowledgments

- **DeepSpeed-Chat**: https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat
- **Meta LLaMA**: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B
- **HuggingFace Transformers**: https://github.com/huggingface/transformers

## ğŸ“ Support

For issues or questions:
1. Check [QUICK_START.md](QUICK_START.md#troubleshooting)
2. Review [TRAINING_CHEATSHEET.md](TRAINING_CHEATSHEET.md)
3. Open an issue on GitHub

---

**Ready to train? Start with [QUICK_START.md](QUICK_START.md)!** ğŸƒğŸ¤–
